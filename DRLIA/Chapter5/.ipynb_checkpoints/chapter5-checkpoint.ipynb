{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 1\n",
    "## Introduction to multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import workers\n",
    "\n",
    "def square(x):\n",
    "    return np.square(x)\n",
    "\n",
    "x = np.arange(64) # sets up an array with a sequence of numbers\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count()) # sets up a multiprocessing processor pool with cpu_count() processes\n",
    "index = 64 / mp.cpu_count()\n",
    "squared = pool.map(square, [x[index * i : index * i + index] for i in range(mp.cpu_count())]) # uses the pool's map function to apply the square function to each array in the list and returns the result in a list\n",
    "squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 2\n",
    "## Manually starting individual processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(i, x, queue):\n",
    "    print(\"In process{}\".format(i, ))\n",
    "    queue.put(np.square(x))\n",
    "    \n",
    "processes = [] # sets up a list to store a reference to each process\n",
    "queue = mp.Queue() # sets up a multiprocessing queue, a data structure that can be shared across processes\n",
    "x = np.arange(64) # sets up some sample data, a sequence of numbers \n",
    "index = 64 / mp.cpu_count()\n",
    "\n",
    "for i in range(mp.cpu_count()): # starts cpu_count() processes with the square function as the target and an individual piece of data to process\n",
    "    start_index = index * i\n",
    "    proc = mp.Process(target=square, args=(i, x[start_index:start_index + index], queue))\n",
    "    proc.start()\n",
    "    processes.append(proc)\n",
    "        \n",
    "for proc in processes: # waits for each process to finish before returning to the main thread\n",
    "    proc.join()\n",
    "    \n",
    "for proc in processes: # terminates each process\n",
    "    proc.terminate()\n",
    "    \n",
    "results = []\n",
    "while not queue.empty(): # converts the multiprocessing queue into a list\n",
    "    results.append(queue.get())\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 3\n",
    "## Pseudocode for online advantage actor-critic"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gamma = 0.9\n",
    "for i in epochs: # iterates over epochs\n",
    "    state = environment.get_state() # gets the current state of the environment\n",
    "    value = critic(state) # predicts the value of the current state\n",
    "    policy = actor(state) # predicts the probability distribution over actions given the state\n",
    "    action = policy.sample() # samples an action from the policy's action distribution\n",
    "    next_state, reward = environment.take_action(action)\n",
    "    value_next = critic(next_state)\n",
    "    advantage = reward + (gamma * value_next - value) # calculates the advantage as the reward plus the difference between the next state value and the current state value\n",
    "    loss = -policy.logprob(action) * advantage # reinforces the action that was just taken based on the advantage\n",
    "    minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 4\n",
    "## CartPole actor-critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import gym\n",
    "import torch.multiprocessing as mp # PyTorch wraps Python's built-in multiprocessing library, and the API is the same\n",
    "\n",
    "class ActorCritic(nn.Module): # defines a single combined model for the actor and critic\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.l1 = nn.Linear(4, 25)\n",
    "        self.l2 = nn.Linear(25, 50)\n",
    "        self.actor_lin1 = nn.Linear(50, 2)\n",
    "        self.l3 = nn.Linear(50, 25)\n",
    "        self.critic_lin1 = nn.Linear(25, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x, dim=0) # normalize the input so that the state values are all within the same range\n",
    "        y = F.relu(self.l1(x))\n",
    "        y = F.relu(self.l2(y))\n",
    "        actor = F.log_softmax(self.actor_lin1(y), dim=0) # the actor head returns the log probabilities over the 2 actions\n",
    "        c = F.relu(self.l3(y.detach())) # detaches the y node so the critic's loss won't backpropagate to layers 1 and 2\n",
    "        critic = torch.tanh(self.critic_lin1(c)) # the critic returns a single number bounded by -1 and +1\n",
    "        return actor, critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 6\n",
    "# The main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(t, worker_model, counter, params):\n",
    "    worker_env = gym.make(\"CartPole-v1\")\n",
    "    worker_env.reset()\n",
    "    worker_opt = optim.Adam(lr=1e-4, params=worker_model.parameters()) # each process runs its own isolated environment and optimizer but shares the model\n",
    "    worker_opt.zero_grad()\n",
    "    for i in range(params['epochs']):\n",
    "        worker_opt.zero_grad()\n",
    "        values, logprobs, rewards = run_episode(worker_env, worker_model) # the run_episode function plays a episode of the game, collecting data along the way\n",
    "        actor_loss, critic_loss, eplen = update_params(worker_opt, values, logprobs, rewards) # uses the collected data from run_episode to run one parameter update step\n",
    "        counter.value = counter.value + 1 # globally shared counter between all the running processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 7\n",
    "## Running an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(worker_env, worker_model):\n",
    "    state = torch.from_numpy(worker_env.env.state).float() # converts the environment state from a numpy array to a PyTorch tensor\n",
    "    values, logprobs, rewards = [], [], [] # creates lists to store the computed state values (critic), log probabilities (actor), and rewards\n",
    "    done = False\n",
    "    j = 0\n",
    "    while done == False: # keeps playing the game until the episode ends\n",
    "        j += 1\n",
    "        policy, value = worker_model(state) # computes the state value and log probabilities over actions\n",
    "        values.append(value)\n",
    "        logits = policy.view(-1)\n",
    "        action_dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = action_dist.sample() # using the actor's log probabilities over actions, creates and samples from a categorical distribution to get an action\n",
    "        logprob_ = policy.view(-1)[action]\n",
    "        logprobs.append(logprob_)\n",
    "        state_, _, done, info = worker_env.step(action.detach().numpy())\n",
    "        state = torch.from_numpy(state_).float()\n",
    "        if done: # if the last action caused the episode to end, sets the reward to -10 and resets the environment\n",
    "            reward = -10\n",
    "            worker_env.reset()\n",
    "        else:\n",
    "            reward = 1.0\n",
    "        rewards.append(reward)\n",
    "    return values, logprobs, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 8\n",
    "## Computing and minimizing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(worker_opt, values, logprobs, rewards, clc=0.1, gamma=0.95):\n",
    "    rewards = torch.Tensor(rewards).flip(dims=(0, )).view(-1) # reverse the order of the rewards, logprobs and values arrays and call .view(-1) to make sure they're flat\n",
    "    logprobs = torch.stack(logprobs).flip(dims=(0, )).view(-1)\n",
    "    values = torch.stack(values).flip(dims=(0, )).view(-1)\n",
    "    Returns = []\n",
    "    ret_ = torch.Tensor([0])\n",
    "    \n",
    "    for r in range(rewards.shape[0]): # for each reward (in reverse order), computes the return value and append it to a returns array\n",
    "        ret_ = rewards[r] + gamma * ret_\n",
    "        Returns.append(ret_)\n",
    "        \n",
    "    Returns = torch.stack(Returns).view(-1)\n",
    "    Returns = F.normalize(Returns, dim=0)\n",
    "    actor_loss = -logprobs * (Returns - values.detach()) # detach the values tensor from the graph to prevent backpropagating through the critic head\n",
    "    critic_loss = torch.pow(values - Returns, 2) # the critic attempts to learn to predict the return\n",
    "    loss = actor_loss.sum() + clc * critic_loss.sum() # sum the actor and critic losses to get an overall loss. Scale down the critic loss by the clc factor\n",
    "    loss.backward()\n",
    "    worker_opt.step()\n",
    "    return actor_loss, critic_loss, len(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 5\n",
    "## Distributing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MasterNode = ActorCritic() # creates a global, shared actor-critic model\n",
    "MasterNode.share_memory() # this method will allow the parameters of the model to be shared across processes rather than being copied\n",
    "\n",
    "processes = [] # sets up a list to store the instantiated processes\n",
    "params = {\n",
    "    'epochs': 1000,\n",
    "    'n_workers': mp.cpu_count() - 1,\n",
    "}\n",
    "counter = mp.Value('i', 0) # a shared global counter using multiprocessing's built-in shared object. The 'i' parameter indicates the type is integer\n",
    "\n",
    "for i in range(params['n_workers']):\n",
    "    p = mp.Process(target=workers.worker, args=(i, MasterNode, counter, params)) # starts a new process that runs the worker function\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "for p in processes: # joins each process to wait for it to finish before returning to the main process\n",
    "    p.join()\n",
    "\n",
    "for p in processes: # makes sure each process is terminated\n",
    "    p.terminate()\n",
    "\n",
    "print(counter.value, processes[0].exitcode) # prints the global counter value and the first process's exit code (which should be 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "eplen = 0\n",
    "steps = []\n",
    "\n",
    "for i in range(1000):\n",
    "    state_ = np.array(env.env.state)\n",
    "    state = torch.from_numpy(state_).float()\n",
    "    logits, value = MasterNode(state)\n",
    "    action_dist = torch.distributions.Categorical(logits=logits)\n",
    "    action = action_dist.sample()\n",
    "    state2, reward, done, info = env.step(action.detach().numpy())\n",
    "    if done:\n",
    "        print(\"Lost\")\n",
    "        env.reset()\n",
    "        steps.append(eplen)\n",
    "        eplen = 0\n",
    "    else:\n",
    "        eplen += 1\n",
    "    state_ = np.array(env.env.state)\n",
    "    state = torch.from_numpy(state_).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N=50):\n",
    "    kernel = np.ones(N)\n",
    "    conv_len = x.shape[0]-N\n",
    "    y = np.zeros(conv_len)\n",
    "    for i in range(conv_len):\n",
    "        y[i] = kernel @ x[i:i+N]\n",
    "        y[i] /= N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "\n",
    "steps = np.array(steps)\n",
    "avg_steps = running_mean(steps, 50)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.ylabel(\"Episode Duration\", fontsize=22)\n",
    "plt.xlabel(\"Training Epochs\", fontsize=22)\n",
    "plt.plot(avg_steps, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 9\n",
    "## N-step training with CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(worker_env, worker_model, N_steps=10):\n",
    "    raw_state = np.array(worker_env.env.state)\n",
    "    state = torch.from_numpy(raw_state).float()\n",
    "    values, logprobs, rewards = [], [], []\n",
    "    done = False\n",
    "    j = 0\n",
    "    G = torch.Tensor([0]) # refers to the return, initialized to 0\n",
    "    \n",
    "    while j < N_steps and done == False: # plays game until N steps or when episode is over\n",
    "        j += 1\n",
    "        policy, value = worker_model(state)\n",
    "        values.append(value)\n",
    "        logits = policy.view(-1)\n",
    "        action_dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = action_dist.sample()\n",
    "        logprob_ = policy.view(-1)[action]\n",
    "        logprobs.append(logprob_)\n",
    "        state_, _, done, info = worker_env.step(action.detach().numpy())\n",
    "        state = torch.from_numpy(state_).float()\n",
    "        if done:\n",
    "            reward = -10\n",
    "            worker_env.reset()\n",
    "        else: # if episode is not done, sets return to the last state value\n",
    "            reward = 1.0\n",
    "            G = value.detach()\n",
    "        rewards.append(reward)\n",
    "    return values, logprobs, rewards, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(worker_opt, values, logprobs, rewards, G, clc=0.1, gamma=0.95):\n",
    "    rewards = torch.Tensor(rewards).flip(dims=(0,)).view(-1)\n",
    "    logprobs = torch.stack(logprobs).flip(dims=(0,)).view(-1)\n",
    "    values = torch.stack(values).flip(dims=(0,)).view(-1)\n",
    "    Returns = []\n",
    "    ret_ = G\n",
    "    \n",
    "    for r in range(rewards.shape[0]): # for each reward (in reverse order), computes the return value and append it to a returns array\n",
    "        ret_ = rewards[r] + gamma * ret_\n",
    "        Returns.append(ret_)\n",
    "        \n",
    "    Returns = torch.stack(Returns).view(-1)\n",
    "    Returns = F.normalize(Returns, dim=0)\n",
    "    actor_loss = -logprobs * (Returns - values.detach()) # detach the values tensor from the graph to prevent backpropagating through the critic head\n",
    "    critic_loss = torch.pow(values - Returns, 2) # the critic attempts to learn to predict the return\n",
    "    loss = actor_loss.sum() + clc * critic_loss.sum() # sum the actor and critic losses to get an overall loss. Scale down the critic loss by the clc factor\n",
    "    loss.backward()\n",
    "    worker_opt.step()\n",
    "    return actor_loss, critic_loss, len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(t, worker_model, counter, params):\n",
    "    worker_env = gym.make(\"CartPole-v1\")\n",
    "    worker_env.reset()\n",
    "    worker_opt = optim.Adam(lr=1e-4, params=worker_model.parameters())\n",
    "    worker_opt.zero_grad()\n",
    "    for i in range(params['epochs']):\n",
    "        worker_opt.zero_grad()\n",
    "        values, logprobs, rewards, G = run_episode(worker_env, worker_model)\n",
    "        actor_loss, critic_loss, eplen = update_params(worker_opt, values, logprobs, rewards, G)\n",
    "        counter.value = counter.value + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "eplen = 0\n",
    "steps = []\n",
    "\n",
    "for i in range(1000):\n",
    "    state_ = np.array(env.env.state)\n",
    "    state = torch.from_numpy(state_).float()\n",
    "    logits, value = MasterNode(state)\n",
    "    action_dist = torch.distributions.Categorical(logits=logits)\n",
    "    action = action_dist.sample()\n",
    "    state2, reward, done, info = env.step(action.detach().numpy())\n",
    "    if done:\n",
    "        print(\"Lost\")\n",
    "        env.reset()\n",
    "        steps.append(eplen)\n",
    "        eplen = 0\n",
    "    else:\n",
    "        eplen += 1\n",
    "    state_ = np.array(env.env.state)\n",
    "    state = torch.from_numpy(state_).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.array(steps)\n",
    "avg_steps = running_mean(steps, 50)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.ylabel(\"Episode Duration\", fontsize=22)\n",
    "plt.xlabel(\"Training Epochs\", fontsize=22)\n",
    "plt.plot(avg_steps, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 10\n",
    "## Returns with and without bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated rewards for 3 steps\n",
    "r1 = [1, 1, -1]\n",
    "r2 = [1, 1, 1]\n",
    "R1, R2 = 0.0, 0.0\n",
    "\n",
    "# No bootstrapping\n",
    "for i in range(len(r1) - 1, 0, -1):\n",
    "    R1 = r1[i] + 0.99 * R1\n",
    "for i in range(len(r2) - 1, 0, -1):\n",
    "    R2 = r2[i] + 0.99 * R2\n",
    "print(\"No bootstrapping\")\n",
    "print(R1, R2)\n",
    "\n",
    "# With bootstrapping\n",
    "R1, R2 = 1.0, 1.0\n",
    "for i in range(len(r1) - 1, 0, -1):\n",
    "    R1 = r1[i] + 0.99 * R1\n",
    "for i in range(len(r2) - 1, 0, -1):\n",
    "    R2 = r2[i] + 0.99 * R2\n",
    "print(\"With bootstrapping\")\n",
    "print(R1, R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
